{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv3/ZdEF8eaXh8mw0o1UWh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS3-JNKHG8yJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "os.listdir\n",
        "from math import*\n",
        "dic = {}\n",
        "k = 0\n",
        "for filename in os.listdir('/content/sample_data/train'):\n",
        "  f = open('/content/sample_data/train/'+filename,'r',encoding='utf-8')\n",
        "  text = f.read()\n",
        "  text = text[:1000].replace('\\n', ' ').lower()\n",
        "  for i in range(len(text)-1):\n",
        "    str = ''\n",
        "    for j in range(2):\n",
        "      str=str+text[i+j]\n",
        "    if str not in dic.keys():\n",
        "      dic[str] = k\n",
        "      k+=1\n",
        "\n",
        "dic = dict(sorted(dic.items(), key=lambda item: item[1]))\n",
        "\n",
        "#print(dic)\n",
        "def into_sentences():\n",
        "  vectors_of_all_sentences = []\n",
        "  counter = 0\n",
        "  d = 0 #счетчик для языков\n",
        "  cluster_mass_centers =[]\n",
        "  for i in range(len(dic)):\n",
        "    cluster_mass_centers.append([0]*len(dic))\n",
        "  for filename in os.listdir('/content/sample_data/train'):\n",
        "    \n",
        "    f = open('/content/sample_data/train/'+filename,'r',encoding='utf-8')\n",
        "    text_ = f.read()\n",
        "    text_ = text_[:1000].replace('\\n', ' ').lower()\n",
        "    #split_regex = re.compile(r'[.|!|?|…]')\n",
        "    #sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(text_)])\n",
        "\n",
        "    sentences = text_.split('.')\n",
        "    sentences = list(filter(None,sentences))\n",
        "    for s in sentences:\n",
        "      #if s == '':\n",
        "      #  del s\n",
        " \n",
        "      v = [0]*len(dic)\n",
        "      for i in range(len(dic)):\n",
        "        if list(dic.keys())[i] in s:\n",
        "          v[i] = s.count(list(dic.keys())[i])\n",
        "          cluster_mass_centers[d][i] += v[i]\n",
        "      v.append(filename)\n",
        "      \n",
        "      d = d +1   \n",
        "      #print(v)\n",
        "      vectors_of_all_sentences.append(v)  \n",
        "  return vectors_of_all_sentences, cluster_mass_centers\n",
        "a,b = into_sentences()\n",
        "def distance(v1, v2):\n",
        "  rasst = 0\n",
        "  for i in range(len(a[0])-2):\n",
        "    rasst += np.linalg.norm(v1[i] - v2[i])\n",
        "  return rasst\n",
        "f = open('/content/sample_data/test/testing.txt','r',encoding='utf-8')\n",
        "vvod = f.read()\n",
        "vvod = vvod[:1000].replace('\\n', ' ').lower()\n",
        "vv = [0]*len(dic)\n",
        "for i in range(len(dic)):\n",
        "    if list(dic.keys())[i] in vvod:\n",
        "      vv[i] = vvod.count(list(dic.keys())[i])\n",
        "distances = [0]*len(a)\n",
        "for i in range(len(a)):\n",
        "  distances[i] = distance(vv, a[i])\n",
        "mins = []\n",
        "distances_num = list(enumerate(distances, 0))\n",
        "t_min = min(distances_num, key=lambda i : i[1])\n",
        "for i in range(27):\n",
        "  t_min = min(distances_num, key=lambda i : i[1])\n",
        "  mins.append(t_min[0])\n",
        "  distances_num.remove(t_min)\n",
        "languages = []\n",
        "for i in range(27):\n",
        "  languages.append(a[mins[i]][-1].replace('.txt', ''))\n",
        "print(max(set(languages), key = languages.count))"
      ]
    }
  ]
}